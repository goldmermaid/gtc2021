{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "yellow-extraction",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### GTC2021 Tutorial S31692\n",
    "----\n",
    "\n",
    "# Dive into Deep Learning:\n",
    "### Code Side-by-Side with MXNet, PyTorch & TensorFlow\n",
    "\n",
    "Speaker: Rachel Hu (AWS AI)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-luxembourg",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "\n",
    "Deep learning is transforming the world nowadays... \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-columbia",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/mxnet_pytorch_tf_transp.png\" alt=\"Drawing\" style=\"width: 400px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-grocery",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "To fulfill the strong wishes of simpler but more practical deep learning materials, [Dive into Deep Learning](https://d2l.ai/), a unified resource of deep learning was born to achieve the following goals:\n",
    "\n",
    "- Adopt the 3 most popular deep leraning frameworks: MXNet, PyTorch and TensorFlow;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-uniform",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Offer depth theory and runnable code, showing readers how to solve problems in practice;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-perth",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Allow for rapid updates, both by us, and also by the community at large;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-butter",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Be complemented by a forum for interactive discussions of technical details and to answer questions;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-competition",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Be freely available for everyone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-danger",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prerequisites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-novel",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### GPU Fundamentals\n",
    "\n",
    "- [Installations with CUDA](https://d2l.ai/chapter_installation/index.html)\n",
    "- [Basic Operations on GPUs](https://nbviewer.jupyter.org/format/slides/github/mli/d2l-1day-notebooks/blob/master/notebooks-2/1-use-gpu.ipynb#/)\n",
    "- [Hardware for deep learning](https://d2l.ai/chapter_computational-performance/hardware.html#gpus-and-other-accelerators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-pregnancy",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deep Learning Fundamentals\n",
    "\n",
    "Here are a few concepts that will be the prerequistes for this lecture. Take a look if some of them are not familiar to you! :)\n",
    "\n",
    "| title                               |  notes    |  slides    |\n",
    "| ------------------------------ | ---- | ---- |\n",
    "| Data Manipulation with Ndarray | [D2L Book](https://d2l.ai/chapter_preliminaries/ndarray.html) | [Jupyter Notebook](https://nbviewer.jupyter.org/format/slides/github/mli/d2l-1day-notebooks/blob/master/notebooks-1/1-ndarray.ipynb#/) |\n",
    "| Multilayer Perceptron (MLP) | [D2L Book](https://d2l.ai/chapter_multilayer-perceptrons/mlp.html) | [Jupyter Notebook](https://nbviewer.jupyter.org/format/slides/github/mli/d2l-1day-notebooks/blob/master/notebooks-1/9-mlp-gluon.ipynb#/) |\n",
    "| Softmax Regression | [D2L Book](https://d2l.ai/chapter_linear-networks/softmax-regression.html) | [Jupyter Notebook](https://nbviewer.jupyter.org/format/slides/github/mli/d2l-1day-notebooks/blob/master/notebooks-1/7-softmax-regression-gluon.ipynb#/) |\n",
    "| Fundamental of Convolutional Neural Networks | [D2L Book](https://d2l.ai/chapter_convolutional-neural-networks/index.html) | [Jupyter Notebook](https://nbviewer.jupyter.org/format/slides/github/goldmermaid/gtc2020/blob/master/Notebooks/convolutions.ipynb) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-water",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Syllabus\n",
    "\n",
    "In this training, we are applying AlexNet directly on Fashion-MNIST dataset, with 3 frameworks side-by-side.\n",
    "\n",
    "| Topics |  |  |\n",
    "| --- | --- | --- |\n",
    "| AlexNet Overview | [D2L Book](https://d2l.ai/chapter_convolutional-modern/alexnet.html#alexnet) | \n",
    "| AlexNet (MXNet) | [Jupyter Notebook](https://nbviewer.jupyter.org/format/slides/github/goldmermaid/gtc2021/blob/master/Notebooks/Alexnet-mxnet.ipynb) | [Nbviewer](notebooks/alexnet-mxnet.slides.html)|\n",
    "| AlexNet (PyTorch) | [Jupyter Notebook](https://nbviewer.jupyter.org/format/slides/github/goldmermaid/gtc2021/blob/master/alexnet-torch.ipynb) | [Nbviewer](notebooks/alexnet-torch.slides.html)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-acrobat",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## AlexNet\n",
    "\n",
    "AlexNet, which employed an 8-layer CNN,\n",
    "won the ImageNet Large Scale Visual Recognition Challenge 2012\n",
    "by a phenomenally large margin.\n",
    "This network showed, for the first time,\n",
    "that the features obtained by learning can transcend manually-designed features, breaking the previous paradigm in computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-tsunami",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The architectures of AlexNet and LeNet are very similar, as we illustrate below.\n",
    "\n",
    "<center><img src=\"https://d2l.ai/_images/alexnet.svg\" alt=\"Drawing\" style=\"width: 300px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-clerk",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note that we provide a slightly streamlined version of AlexNet\n",
    "removing some of the design quirks that were needed in 2012\n",
    "to make the model fit on two small GPUs.\n",
    "\n",
    "\n",
    "The design philosophies of AlexNet and LeNet are very similar,\n",
    "but there are also significant differences.\n",
    "First, AlexNet is much deeper than the comparatively small LeNet5.\n",
    "AlexNet consists of eight layers: five convolutional layers,\n",
    "two fully-connected hidden layers, and one fully-connected output layer. Second, AlexNet used the ReLU instead of the sigmoid\n",
    "as its activation function.\n",
    "Let us delve into the details below.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "In AlexNet's first layer, the convolution window shape is $11\\times11$.\n",
    "Since most images in ImageNet are more than ten times higher and wider\n",
    "than the MNIST images,\n",
    "objects in ImageNet data tend to occupy more pixels.\n",
    "Consequently, a larger convolution window is needed to capture the object.\n",
    "The convolution window shape in the second layer\n",
    "is reduced to $5\\times5$, followed by $3\\times3$.\n",
    "In addition, after the first, second, and fifth convolutional layers,\n",
    "the network adds maximum pooling layers\n",
    "with a window shape of $3\\times3$ and a stride of 2.\n",
    "Moreover, AlexNet has ten times more convolution channels than LeNet.\n",
    "\n",
    "After the last convolutional layer there are two fully-connected layers\n",
    "with 4096 outputs.\n",
    "These two huge fully-connected layers produce model parameters of nearly 1 GB.\n",
    "Due to the limited memory in early GPUs,\n",
    "the original AlexNet used a dual data stream design,\n",
    "so that each of their two GPUs could be responsible\n",
    "for storing and computing only its half of the model.\n",
    "Fortunately, GPU memory is comparatively abundant now,\n",
    "so we rarely need to break up models across GPUs these days\n",
    "(our version of the AlexNet model deviates\n",
    "from the original paper in this aspect).\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "Besides, AlexNet changed the sigmoid activation function to a simpler ReLU activation function. On one hand, the computation of the ReLU activation function is simpler. For example, it does not have the exponentiation operation found in the sigmoid activation function.\n",
    " On the other hand, the ReLU activation function makes model training easier when using different parameter initialization methods. This is because, when the output of the sigmoid activation function is very close to 0 or 1, the gradient of these regions is almost 0, so that backpropagation cannot continue to update some of the model parameters. In contrast, the gradient of the ReLU activation function in the positive interval is always 1. Therefore, if the model parameters are not properly initialized, the sigmoid function may obtain a gradient of almost 0 in the positive interval, so that the model cannot be effectively trained.\n",
    "\n",
    "### Capacity Control and Preprocessing\n",
    "\n",
    "AlexNet controls the model complexity of the fully-connected layer\n",
    "by [dropout](https://d2l.ai/chapter_multilayer-perceptrons/dropout.html),\n",
    "while LeNet only uses weight decay.\n",
    "To [augment the data](https://d2l.ai/chapter_computer-vision/image-augmentation.html) even further, the training loop of AlexNet\n",
    "added a great deal of image augmentation,\n",
    "such as flipping, clipping, and color changes.\n",
    "This makes the model more robust and the larger sample size effectively reduces overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-differential",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coding side-by-side\n",
    "\n",
    "| Topics |  |\n",
    "| --- | --- |\n",
    "| AlexNet Overview | [D2L Book](https://d2l.ai/chapter_convolutional-modern/alexnet.html#alexnet) |\n",
    "| AlexNet (MXNet) | [Jupyter Notebook](https://nbviewer.jupyter.org/format/slides/github/goldmermaid/gtc2021/blob/master/Notebooks/Alexnet-mxnet.ipynb) |\n",
    "| AlexNet (PyTorch) | [Jupyter Notebook](https://nbviewer.jupyter.org/format/slides/github/goldmermaid/gtc2021/blob/master/alexnet-torch.ipynb) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-means",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bonus Resources\n",
    "\n",
    "- [AutoGluon](https://autogluon.mxnet.io/) enables easy-to-use and easy-to-extend AutoML with a focus on deep learning and real-world applications spanning image, text, or tabular data;\n",
    "\n",
    "- [Deep Graph Libray](https://www.dgl.ai/) develops easy-to-use, high performance and scalable Python package for deep learning on graphs;\n",
    "\n",
    "- [GluonTS](https://gluon-ts.mxnet.io/) supports deep learning based probabilistic time series modeling;\n",
    "\n",
    "- [TVM](https://tvm.apache.org/): automatic generates and optimizes tensor operators on more backend with better performance for CPUs, GPUs and specialized accelerators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-encoding",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Q&A \n",
    "If you have any question, please leave us a message at our [discussion forum](https://discuss.d2l.ai/). Have fun diving into deep learning!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
